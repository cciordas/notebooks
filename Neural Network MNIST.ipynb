{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global functions used by the neural network.\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def D_sigmoid(z):\n",
    "    \"\"\"The derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def D_cost(output, target):\n",
    "    \"\"\"The derivative of the quadratic cost function.\"\"\"\n",
    "    return (output - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkMNIST(object):\n",
    "    \"\"\"\n",
    "    A Neural Network designed to be trained on the MNIST database. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        Initializes biases and weights with random values from a N(0,1) distribution.\n",
    "        Since we are using this network with the MNIST database, the input layer must\n",
    "        be of size 784 = 28 x 28 (the number of pixels of each image). Also, the ouput\n",
    "        layer must be of size 10 (to represent 0...9)\n",
    "\n",
    "        Parameters:\n",
    "        layers -- list of layer sizes, layers[0] corresponding to the input layer\n",
    "        \"\"\"\n",
    "        if layers [0] != 784: raise RuntimeError('The size of the input layer must be 784')\n",
    "        if layers[-1] !=  10: raise RuntimeError('The size of the output layer must be 10')\n",
    "        \n",
    "        self.nlayers = len(layers)\n",
    "        self.biases  = [np.random.randn(i, 1) for i in layers[1:]]\n",
    "        self.weights = [np.random.randn(j, i) for i, j in zip(layers[:-1], layers[1:])]\n",
    "\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        Propagates a given input vector through the network and returns the output.\n",
    "        \n",
    "        Parameters:\n",
    "        a -- network input, a 2D array of shape (n,1), where n is the input layer size        \n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def backpropagate(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost function for a given input vector \n",
    "        using the backpropagation algorithm.\n",
    "        \n",
    "        Parameters:\n",
    "        x -- the input vector\n",
    "        y -- the target output corresponding to the given input\n",
    "        \n",
    "        Returns:\n",
    "        A tuple consisting of two lists.\n",
    "        The first  one stores the derivative of the cost function with respect to the bias.\n",
    "        The second one stores the derivative of the cost function with respect to the weights.\n",
    "        In each list, the entries are arrays, and correspond to the network layers.\n",
    "        For example, the first entry in the first list is the 1D array of derivatives of the\n",
    "        cost function with respect to the biases for the neurons in the input layer.\n",
    "        Similarly, the first entry in the second list is the 2D array of derivatives of the cost\n",
    "        function with respect to the weights associated with the connections between the neurons\n",
    "        in the input and first hidden layers.\n",
    "        \"\"\"\n",
    "        \n",
    "        # create arrays to store the gradient of the cost function \n",
    "        dC_dBias   = [np.zeros(b.shape) for b in self.biases ]   # <-- derivs of C with respect to biases\n",
    "        dC_dWeight = [np.zeros(w.shape) for w in self.weights]   # <-- derivs of C with respect to weights\n",
    "        \n",
    "        # -- FORWARD PASS\n",
    "        \n",
    "        activation  =  x\n",
    "        activations = [x]\n",
    "        zs          = [ ]    \n",
    "        \n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)            \n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # -- BACKWARD PASS\n",
    "        \n",
    "        # output layer\n",
    "        delta          = D_cost(activations[-1], y) * D_sigmoid(zs[-1])  # <-- deriv of 'C' with respect to 'z'\n",
    "        dC_dBias  [-1] = delta\n",
    "        dC_dWeight[-1] = np.dot(delta, activations[-2].T)\n",
    "               \n",
    "        # backpropagate\n",
    "        for L in range(2, self.nlayers):           \n",
    "            delta          = np.dot(self.weights[-L+1].T, delta) * D_sigmoid(zs[-L])\n",
    "            dC_dBias  [-L] = delta\n",
    "            dC_dWeight[-L] = np.dot(delta, activations[-L-1].T)\n",
    "            \n",
    "        return (dC_dBias, dC_dWeight)                               \n",
    "                                \n",
    "        \n",
    "    def SGD(self, training_data, epochs, batchsz, eta, test_data=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using batch stochastic gradient descent.  \n",
    "        The network weights and biases are updated as the result of running this method.\n",
    "        Both 'training_data' and 'test_data' are lists of tuples, each tuple being a\n",
    "        training example - the first element is the network input, the second is the\n",
    "        target output.\n",
    "        \n",
    "        Parameters:\n",
    "        training_data -- list of tuples representing training inputs and the desired outputs  \n",
    "        epochs        -- for how many epochs to train the network\n",
    "        batchsz       -- the size of each batch of training example (this is *stochastic* GD)\n",
    "        eta           -- the learning rate\n",
    "        test_data     -- used to evaluate the performace of the network at the end of each epoch\n",
    "        \"\"\"\n",
    "        for j in range(epochs):\n",
    "            \n",
    "            # break up the training data into batches\n",
    "            np.random.shuffle(training_data)\n",
    "            batches = [training_data[k:k+batchsz] for k in range(0, len(training_data), batchsz)]\n",
    "            \n",
    "            # SGD means that we update weights/biases based on gradients calculated\n",
    "            # using only a batch of training examples (as opposed to the entire training data) \n",
    "            for batch in batches:\n",
    "                dC_dBias_sum   = [np.zeros(b.shape) for b in self.biases ]\n",
    "                dC_dWeight_sum = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "                # calculate the (stochastic) gradient\n",
    "                # below 'x' represents an input vector, 'y' the target output\n",
    "                for x, y in batch:\n",
    "                    dC_dBias, dC_dWeight = self.backpropagate(x, y)\n",
    "                    dC_dBias_sum   = [bs+b for bs, b in zip(dC_dBias_sum  , dC_dBias)  ]\n",
    "                    dC_dWeight_sum = [ws+w for ws, w in zip(dC_dWeight_sum, dC_dWeight)]\n",
    "\n",
    "                # update weights/biases in the direction of the stochastic gradient\n",
    "                eta_avg = eta/len(batch)\n",
    "                self.weights = [w - eta_avg * deriv_w for w, deriv_w in zip(self.weights, dC_dWeight_sum)]\n",
    "                self.biases  = [b - eta_avg * deriv_b for b, deriv_b in zip(self.biases , dC_dBias_sum)  ]\n",
    "            \n",
    "            if test_data: print(\"Epoch %2d: %d of %d\" % (j+1, self.evaluate(test_data), len(test_data)))\n",
    "            else:         print(\"Epoch %2d complete\"  % (j+1))\n",
    "\n",
    "                        \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Evaluates the performance of the neural network on a data set not used for training.\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the MNIST database...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# where to find the file storing the MNIST database\n",
    "MNIST_DATA_FILEPATH = \"mnist.pkl.gz\"\n",
    "\n",
    "def load_data_raw():\n",
    "    \"\"\"\n",
    "    Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The 'training_data' is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the 'training_data' tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The 'validation_data' and 'test_data' are similar, except\n",
    "    each contains only 10,000 images.\n",
    "    \"\"\"\n",
    "    f = gzip.open(MNIST_DATA_FILEPATH, 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Repackages the data returned by 'load_data_raw' in a format\n",
    "    more convenient for using with the neural network.\n",
    "    \n",
    "    Return a tuple (training_data, validation_data, test_data).\n",
    "\n",
    "    'training_data'   is a list of 50,000 2-tuples (x, y)\n",
    "    'validation_data' is a list of 10,000 2-tuples (x, z)\n",
    "    'test_data'       is a list of 10,000 2-tuples (x, z)\n",
    "\n",
    "    'x' is a numpy array of shape (784, 1) containing the input image.\n",
    "    'y' is a numpy array of shape (10, 1) representing the digit represented\n",
    "        by 'x' (it has 0 entries with the exception of one 1 in the position\n",
    "        of the digit represented by 'x')\n",
    "    'z' is just the digit represented by 'x'\n",
    "    \"\"\"\n",
    "    tr_d, va_d, te_d = load_data_raw()\n",
    "\n",
    "    training_inputs   = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results  = [asvector(y) for y in tr_d[1]]\n",
    "    training_data     = zip(training_inputs, training_results)\n",
    "\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data   = zip(validation_inputs, va_d[1])\n",
    "    \n",
    "    test_inputs       = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data         = zip(test_inputs, te_d[1])\n",
    "\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "\n",
    "def asvector(j):\n",
    "    \"\"\"Create vector of shape (10, 1) with 1.0 in the jth position and 0.0 elsewhere.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "# load the MNIST databse\n",
    "print(\"Loading the MNIST database...\")\n",
    "training_data, validation_data, test_data = [list(d) for d in load_data()]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained network: got right 932 out of 10000 (accuracy 9.32 pct)\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetworkMNIST([784, 30, 10])\n",
    "\n",
    "# display the performance of the untrained network\n",
    "nright = net.evaluate(test_data)\n",
    "print(\"Untrained network: got right %d out of %d (accuracy %.2f pct)\" % (nright, len(test_data), 100*float(nright)/len(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: 8160 of 10000\n",
      "Epoch  2: 8341 of 10000\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "\n",
    "EPOCHS  = 10\n",
    "BATCHSZ = 10\n",
    "ETA     =  2\n",
    "\n",
    "net.SGD(training_data, EPOCHS, BATCHSZ, ETA, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "c5bf1c3c-2184-4f4d-b3b9-1da490978f24",
    "theme": {
     "31fa1655-de8b-4611-9383-9b5d16c1d4fa": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "31fa1655-de8b-4611-9383-9b5d16c1d4fa",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     },
     "c5bf1c3c-2184-4f4d-b3b9-1da490978f24": {
      "id": "c5bf1c3c-2184-4f4d-b3b9-1da490978f24",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         155,
         177,
         192
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410"
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 8
       },
       "h2": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "font-family": "Merriweather",
       "font-size": 4
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
