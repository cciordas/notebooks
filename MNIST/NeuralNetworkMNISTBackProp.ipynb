{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Neural Network with the Backpropagation algorithm\n",
    "<br\\>\n",
    "This notebook implements 'from scratch' a Neural Network for recognizing hand written digits, trained using the MNIST database. By 'from scratch' we mean that the backpropagation algorithm is implemented explicitely, without resorting to automatic differentiation techniques. We use `numpy` but not any other libraries (like `tensorflow` or `pytorch`).\n",
    "<br\\><br\\>\n",
    "Also see the `NeuralNetworkMNISTAutoDiff` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import pickle\n",
    "import timeit\n",
    "import line_profiler\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global functions used by the neural network.\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def D_sigmoid(z):\n",
    "    \"\"\"The derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def D_cost(output, target):\n",
    "    \"\"\"The derivative of the quadratic cost function.\"\"\"\n",
    "    return (output - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkMNISTBackProp(object):\n",
    "    \"\"\"\n",
    "    A Neural Network designed to be used with the MNIST database. \n",
    "    Gradients are calculated using the backpropagation algorithm,\n",
    "    which is implemented explicitely.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layersz):\n",
    "        \"\"\"\n",
    "        Pass in a list of layer sizes (layersz[0]/layersz[-1] are the input/output layers).\n",
    "        The size of this list is the number of layers in the network.\n",
    "        Since we are using this network with the MNIST database, the input layer must\n",
    "        be of size 784 = 28 x 28 (the number of pixels of each image). Also, the ouput\n",
    "        layer must be of size 10 (to represent 0...9).\n",
    "        \n",
    "        PARAMETERS:\n",
    "        layersz -- list of layer sizes\n",
    "        \"\"\"\n",
    "        if layersz [0] != 784: raise RuntimeError('The size of the input layer must be 784')\n",
    "        if layersz[-1] !=  10: raise RuntimeError('The size of the output layer must be 10')\n",
    "        \n",
    "        self.nlayers = len(layersz)\n",
    "        self.layersz = layersz\n",
    "\n",
    "        # Initializes biases and weights with random values from a N(0,1) distribution.\n",
    "        # The following convention is used for weights: \n",
    "        #    w[i,j] denotes the weight associated with the connection from neuron\n",
    "        #   'j' in the previous layer to the neuron 'i' in the current layer.\n",
    "                        \n",
    "        self.biases  = [np.random.randn(i, 1) for i in layersz[1:]]\n",
    "        self.weights = [np.random.randn(i, j) for i, j in zip(layersz[1:], layersz[:-1])]\n",
    "\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        Propagates a given input vector forward through the network and returns the output.\n",
    "        PARAMETERS:\n",
    "        an ndarray of shape (784,1), representing a digit image from the MNIST database\n",
    "        RETURN:\n",
    "        an ndarray of shape (10, 1), a one-hot representation of the network output (0..9)\n",
    "        \"\"\"\n",
    "        if a.shape != (self.layersz[0], 1):\n",
    "            raise RuntimeError('Input array has wrong shape - must be (784, 1)')\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def backpropagate(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the quadratic cost function with respect to the network\n",
    "        weights and biases for a given input vector using the backpropagation algorithm.\n",
    "        \n",
    "        PARAMETERS:\n",
    "        x -- an ndarray of shape (784,1), representing a digit image from the MNIST db\n",
    "        y -- an ndarray of shape ( 10,1), one-hot encoding of digit represented by the image\n",
    "        \n",
    "        RETURN:\n",
    "        A tuple consisting of two lists.  \n",
    "        The first  list stores the derivative of the cost function with respect to biases.\n",
    "        The second list stores the derivative of the cost function with respect to weights.\n",
    "        Each list has one entry for each network layer. Each entry is an ndarray.\n",
    "\n",
    "        For example, the first entry in the first list is the array of derivatives of the\n",
    "        cost function with respect to the biases for the neurons in the input layer.\n",
    "        Similarly, the first entry in the second list is the array of derivatives of the cost\n",
    "        function with respect to the weights associated with the connections between the neurons\n",
    "        in the input and first hidden layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if x.shape != (self.layersz[0] , 1): raise RuntimeError( 'Input array has wrong shape')\n",
    "        if y.shape != (self.layersz[-1], 1): raise RuntimeError('Output array has wrong shape')\n",
    "        \n",
    "        # create arrays to store the gradient of the cost function \n",
    "        dC_dBias   = [np.zeros(b.shape) for b in self.biases ]   # <-- derivs of C with respect to biases\n",
    "        dC_dWeight = [np.zeros(w.shape) for w in self.weights]   # <-- derivs of C with respect to weights\n",
    "        \n",
    "        # -- FORWARD PASS\n",
    "        \n",
    "        activation  =  x\n",
    "        activations = [x]\n",
    "        zs          = [ ]    \n",
    "        \n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)            \n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # -- BACKWARD PASS\n",
    "        \n",
    "        # output layer (assume a quadratic cost function: the square of the norm of the vector \n",
    "        # difference between the one-hot vectors represening the network output and the correct answer)\n",
    "        delta          = D_cost(activations[-1], y) * D_sigmoid(zs[-1])  # <-- deriv of 'C' with respect to 'z'\n",
    "        dC_dBias  [-1] = delta\n",
    "        dC_dWeight[-1] = np.dot(delta, activations[-2].T)\n",
    "               \n",
    "        # backpropagate\n",
    "        for L in range(2, self.nlayers):           \n",
    "            delta          = np.dot(self.weights[-L+1].T, delta) * D_sigmoid(zs[-L])\n",
    "            dC_dBias  [-L] = delta\n",
    "            dC_dWeight[-L] = np.dot(delta, activations[-L-1].T)\n",
    "            \n",
    "        return (dC_dBias, dC_dWeight)                               \n",
    "                                \n",
    "        \n",
    "    def SGD(self, training_data, epochs, batchsz, eta, test_data=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using batch stochastic gradient descent.  \n",
    "        The network weights and biases are updated as the result of running this method.\n",
    "        Both 'training_data' and 'test_data' are lists of tuples, each tuple being an\n",
    "        example - the first element is the network input, the second is the target output.\n",
    "        For both data sets, the first tuple element is an MNIST digit image, represented \n",
    "        as an (784, 1) ndarray.\n",
    "        The target output (the digit associated with the image) is represented as a one-hot\n",
    "        ndarray of shape (10, 1) in 'training_data', and as the actual digit (0..9) in \n",
    "        'test_data'.\n",
    "        \n",
    "        PARAMETERS:\n",
    "        training_data -- list of tuples representing training inputs and the desired outputs  \n",
    "        epochs        -- for how many epochs to train the network\n",
    "        batchsz       -- the size of each batch of training example (this is *stochastic* GD)\n",
    "        eta           -- the learning rate\n",
    "        test_data     -- used to evaluate the performace of the network at the end of each epoch\n",
    "        \"\"\"\n",
    "        for j in range(epochs):\n",
    "            start_time = timeit.default_timer()\n",
    "                \n",
    "            # break up the training data into batches\n",
    "            np.random.shuffle(training_data)\n",
    "            batches = [training_data[k:k+batchsz] for k in range(0, len(training_data), batchsz)]\n",
    "            \n",
    "            # SGD means that we update weights/biases based on gradients calculated using\n",
    "            # only a batch of training examples (as opposed to the entire training dataset) \n",
    "            for batch in batches:\n",
    "                dC_dBias_sum   = [np.zeros(b.shape) for b in self.biases ]\n",
    "                dC_dWeight_sum = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "                # calculate the (stochastic) gradient\n",
    "                # below 'x' represents an input image, 'y' the associated digit\n",
    "                for x, y in batch:\n",
    "                    dC_dBias, dC_dWeight = self.backpropagate(x, y)\n",
    "                    dC_dBias_sum   = [bs+b for bs, b in zip(dC_dBias_sum  , dC_dBias)  ]\n",
    "                    dC_dWeight_sum = [ws+w for ws, w in zip(dC_dWeight_sum, dC_dWeight)]\n",
    "                    \n",
    "                # update weights/biases in the direction of the stochastic gradient\n",
    "                eta_scaled = float(eta)/len(batch)\n",
    "                self.weights = [w - eta_scaled * deriv_w for w, deriv_w in zip(self.weights, dC_dWeight_sum)]\n",
    "                self.biases  = [b - eta_scaled * deriv_b for b, deriv_b in zip(self.biases , dC_dBias_sum)  ]\n",
    "                            \n",
    "            dt = timeit.default_timer() - start_time\n",
    "            if test_data: print(\"Epoch %2d: %d of %d (elapsed time: %fs)\" % (j+1, self.evaluate(test_data), len(test_data), dt))\n",
    "            else:         print(\"Epoch %2d complete  (elapsed time: %fs)\" % (j+1), dt)\n",
    "\n",
    "                        \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Evaluates the performance of the neural network on a given data set.\n",
    "        This dataset consists of a list of tuples, each tuple being an example: \n",
    "        the first tuple entry is an image encoded as an ndarray of shape (784, 1),\n",
    "        the second one is the digit the image represents (as 0..9). \n",
    "        PARAMETERS:\n",
    "        test_data -- dataset used for evaluating network performance\n",
    "        RETURNS:\n",
    "        number of correct answers on the given dataset\n",
    "        \"\"\"\n",
    "        # when passing an image through the network, the output is a one-hot vector\n",
    "        # we use the 'argmax' to convert this vector to the 0..9 digit it represents\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the MNIST database...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# where to find the file storing the MNIST database\n",
    "MNIST_DATA_FILEPATH = \"mnist.pkl.gz\"\n",
    "\n",
    "def load_data_raw():\n",
    "    \"\"\"\n",
    "    Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The 'training_data' is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the 'training_data' tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The 'validation_data' and 'test_data' are similar, except\n",
    "    each contains only 10,000 images.\n",
    "    \"\"\"\n",
    "    f = gzip.open(MNIST_DATA_FILEPATH, 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Repackages the data returned by 'load_data_raw' in a format\n",
    "    more convenient for using with the neural network.\n",
    "    \n",
    "    Return a tuple (training_data, validation_data, test_data).\n",
    "\n",
    "    'training_data'   is a list of 50,000 2-tuples (x, y)\n",
    "    'validation_data' is a list of 10,000 2-tuples (x, z)\n",
    "    'test_data'       is a list of 10,000 2-tuples (x, z)\n",
    "\n",
    "    'x' is a numpy array of shape (784, 1) containing the input image.\n",
    "    'y' is a numpy array of shape ( 10, 1) representing the digit encoded\n",
    "        by 'x' (it has 0 entries with the exception of one 1 in the position\n",
    "        of the digit represented by 'x')\n",
    "    'z' is just the digit represented by 'x'\n",
    "    \"\"\"\n",
    "    tr_d, va_d, te_d = load_data_raw()\n",
    "\n",
    "    training_inputs   = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results  = [asvector(y) for y in tr_d[1]]\n",
    "    training_data     = zip(training_inputs, training_results)\n",
    "\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data   = zip(validation_inputs, va_d[1])\n",
    "    \n",
    "    test_inputs       = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data         = zip(test_inputs, te_d[1])\n",
    "\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "\n",
    "def asvector(j):\n",
    "    \"\"\"Create vector of shape (10, 1) with 1.0 in the jth position and 0.0 elsewhere.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "# load the MNIST databse\n",
    "print(\"Loading the MNIST database...\")\n",
    "training_data, validation_data, test_data = [list(d) for d in load_data()]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a Neural Network\n",
    "\n",
    "layer_sizes = [784, 30, 10]  # <-- first entry must be 784, last one must be 10\n",
    "np.random.seed(1234)         # <-- for reproducible results (see also comments below) \n",
    "net = NeuralNetworkMNISTBackProp(layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance of the untrained network\n",
    "\n",
    "nright = net.evaluate(test_data)\n",
    "print(\"Untrained network: got right %d out of %d (accuracy %.2f pct)\" % (nright, len(test_data), 100*float(nright)/len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the network, the following params result in ~94.5% accurancy.<br/>\n",
    "```\n",
    "EPOCHS  = 20 \n",
    "BATCHSZ = 10 \n",
    "ETA     =  2 \n",
    "np.random.seed(1234)\n",
    "```\n",
    "We seed the random number generator explicitely to ensure reproducible results.\n",
    "While most of the time the network will converge to the quoted accuracy, once in a while it can get stuck around 80% and refuse to improve (the only thing that differs between these runs is the random number init).\n",
    "Notice that seeding the RNG must happen before we instantiate the network (because the weights are init to random values in the constructor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: 8947 of 10000 (elapsed time: 5.303515s)\n",
      "Epoch  2: 9181 of 10000 (elapsed time: 5.250960s)\n",
      "Epoch  3: 9240 of 10000 (elapsed time: 5.210710s)\n",
      "Epoch  4: 9280 of 10000 (elapsed time: 5.257962s)\n",
      "Epoch  5: 9344 of 10000 (elapsed time: 5.215868s)\n",
      "Epoch  6: 9373 of 10000 (elapsed time: 5.722547s)\n",
      "Epoch  7: 9412 of 10000 (elapsed time: 5.171901s)\n",
      "Epoch  8: 9421 of 10000 (elapsed time: 5.130221s)\n",
      "Epoch  9: 9404 of 10000 (elapsed time: 5.199178s)\n",
      "Epoch 10: 9424 of 10000 (elapsed time: 5.128226s)\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "\n",
    "EPOCHS  = 10\n",
    "BATCHSZ = 10\n",
    "ETA     =  2\n",
    "\n",
    "net.SGD(training_data, EPOCHS, BATCHSZ, ETA, test_data)\n",
    "\n",
    "# to profile the network, run the line below instead of the one above\n",
    "# set EPOCHS = 1\n",
    "#%lprun -f net.SGD net.SGD(training_data, EPOCHS, BATCHSZ, ETA, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "c5bf1c3c-2184-4f4d-b3b9-1da490978f24",
    "theme": {
     "31fa1655-de8b-4611-9383-9b5d16c1d4fa": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "31fa1655-de8b-4611-9383-9b5d16c1d4fa",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     },
     "c5bf1c3c-2184-4f4d-b3b9-1da490978f24": {
      "id": "c5bf1c3c-2184-4f4d-b3b9-1da490978f24",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         155,
         177,
         192
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410"
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 8
       },
       "h2": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "font-family": "Merriweather",
       "font-size": 4
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
