{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkMNIST(object):\n",
    "    \"\"\"\n",
    "    A Neural Network designed to be trained on the MNIST database. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layersz):\n",
    "        \"\"\"\n",
    "        Initializes biases and weights with random values from a N(0,1) distribution.\n",
    "        Since we are using this network with the MNIST database, the input layer must\n",
    "        be of size 784 = 28 x 28 (the number of pixels of each image). Also, the ouput\n",
    "        layer must be of size 10 (to represent 0...9)\n",
    "\n",
    "        Parameters:\n",
    "        layersz -- list of layer sizes, layers[0] corresponding to the input layer\n",
    "        \"\"\"\n",
    "        if layersz [0] != 784: raise RuntimeError('The size of the input layer must be 784')\n",
    "        if layersz[-1] !=  10: raise RuntimeError('The size of the output layer must be 10')\n",
    "        \n",
    "        # the following convention is used for weights: \n",
    "        # w[i,j] denotes the weight associated with the connection from neuron\n",
    "        # 'j' in the previous layer to the neuron 'i' in the current layer.\n",
    "        \n",
    "        self.nlayers = len(layersz)\n",
    "        self.layersz = layersz\n",
    "        \n",
    "        # we init via the numpy random number generator so we can compare with other implementation\n",
    "        # for debugging purposes\n",
    "        bs = [np.random.randn(i, 1) for i in layersz[1:]]\n",
    "        ws = [np.random.randn(j, i) for i, j in zip(layersz[:-1], layersz[1:])]\n",
    "\n",
    "        self.biases  = [Variable(torch.from_numpy(b).float(), requires_grad=True) for b in bs]\n",
    "        self.weights = [Variable(torch.from_numpy(w).float(), requires_grad=True) for w in ws]\n",
    "\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        Propagates a given input vector forward through the network and returns the output.\n",
    "        \n",
    "        Parameters:\n",
    "        a -- network input, a 2D array of shape (n,1), where n is the size of the input layer        \n",
    "        \"\"\"\n",
    "        if a.data.shape != (self.layersz[0], 1):\n",
    "            raise RuntimeError('Input tensor has wrong shape')\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = torch.sigmoid(torch.mm(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def SGD(self, training_data, epochs, batchsz, eta, test_data=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using batch stochastic gradient descent.  \n",
    "        The network weights and biases are updated as the result of running this method.\n",
    "        Both 'training_data' and 'test_data' are lists of tuples, each tuple being an\n",
    "        example - the first element is the network input, the second is the target output.\n",
    "        \n",
    "        Parameters:\n",
    "        training_data -- list of tuples representing training inputs and the desired outputs  \n",
    "        epochs        -- for how many epochs to train the network\n",
    "        batchsz       -- the size of each batch of training example (this is *stochastic* GD)\n",
    "        eta           -- the learning rate\n",
    "        test_data     -- used to evaluate the performace of the network at the end of each epoch\n",
    "        \"\"\"\n",
    "        for j in range(epochs):\n",
    "            start_time = timeit.default_timer()\n",
    "                \n",
    "            # break up the training data into batches\n",
    "            np.random.shuffle(training_data)\n",
    "            batches = [training_data[k:k+batchsz] for k in range(0, len(training_data), batchsz)]\n",
    "            \n",
    "            # SGD means that we update weights/biases based on gradients calculated\n",
    "            # using only a batch of training examples (as opposed to the entire training data) \n",
    "            for batch in batches:\n",
    "                for b in self.biases:  \n",
    "                    if b.grad is not None: \n",
    "                        b.grad.data.zero_() \n",
    "                for w in self.weights: \n",
    "                    if w.grad is not None: \n",
    "                        w.grad.data.zero_() \n",
    "                \n",
    "                # calculate the (stochastic) gradient\n",
    "                # below 'x' represents an input vector, 'y' the target output\n",
    "                #t0 = timeit.default_timer()\n",
    "                for x, y in batch:\n",
    "                    out  = self.feedforward(x)\n",
    "                    loss = (out - y).pow(2).sum()  # <-- quadratic loss function\n",
    "                    loss.backward()\n",
    "                #print(timeit.default_timer() - t0)\n",
    "                        \n",
    "                # update weights/biases in the direction of the stochastic gradient\n",
    "                eta_avg = eta/len(batch)\n",
    "                for w in self.weights:\n",
    "                    w.data = w.data - eta_avg * w.grad.data\n",
    "                for b in self.biases:\n",
    "                    b.data = b.data - eta_avg * b.grad.data\n",
    "                                    \n",
    "            dt = timeit.default_timer() - start_time\n",
    "            if test_data: print(\"Epoch %2d: %d of %d (elapsed time: %fs)\" % (j+1, self.evaluate(test_data), len(test_data), dt))\n",
    "            else:         print(\"Epoch %2d complete  (elapsed time: %fs)\" % (j+1), dt)\n",
    "\n",
    "                \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Evaluates the performance of the neural network on a data set not used for training.\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x).data.numpy()), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y.data.numpy()) for (x, y) in test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the MNIST database...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# where to find the file storing the MNIST database\n",
    "MNIST_DATA_FILEPATH = \"mnist.pkl.gz\"\n",
    "\n",
    "def load_data_raw():\n",
    "    \"\"\"\n",
    "    Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The 'training_data' is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the 'training_data' tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The 'validation_data' and 'test_data' are similar, except\n",
    "    each contains only 10,000 images.\n",
    "    \"\"\"\n",
    "    f = gzip.open(MNIST_DATA_FILEPATH, 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Repackages the data returned by 'load_data_raw' in a format\n",
    "    more convenient for using with the neural network.\n",
    "    \n",
    "    Return a tuple (training_data, validation_data, test_data).\n",
    "\n",
    "    'training_data'   is a list of 50,000 2-tuples (x, y)\n",
    "    'validation_data' is a list of 10,000 2-tuples (x, z)\n",
    "    'test_data'       is a list of 10,000 2-tuples (x, z)\n",
    "\n",
    "    'x' is a numpy array of shape (784, 1) containing the input image.\n",
    "    'y' is a numpy array of shape ( 10, 1) representing the digit encoded\n",
    "        by 'x' (it has 0 entries with the exception of one 1 in the position\n",
    "        of the digit represented by 'x')\n",
    "    'z' is just the digit represented by 'x'\n",
    "    \"\"\"\n",
    "    tr_d, va_d, te_d = load_data_raw()\n",
    "\n",
    "    training_inputs  = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_inputs  = [Variable(torch.from_numpy(arr).float(), requires_grad=False) for arr in training_inputs]\n",
    "    \n",
    "    training_results = [asvector(y) for y in tr_d[1]]\n",
    "    training_results = [Variable(torch.from_numpy(arr).float(), requires_grad=False) for arr in training_results]\n",
    "\n",
    "    training_data    = zip(training_inputs, training_results)\n",
    "\n",
    "    # ------------\n",
    "    \n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_inputs = [Variable(torch.from_numpy(arr).float(), requires_grad=False) for arr in validation_inputs]    \n",
    "    validation_data   = zip(validation_inputs, Variable(torch.from_numpy(va_d[1]).float(), requires_grad=False))\n",
    "\n",
    "    # ------------\n",
    "\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_inputs = [Variable(torch.from_numpy(arr).float(), requires_grad=False) for arr in test_inputs]\n",
    "    test_data   = zip(test_inputs, Variable(torch.from_numpy(te_d[1]).float(), requires_grad=False))\n",
    "\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "\n",
    "def asvector(j):\n",
    "    \"\"\"Create vector of shape (10, 1) with 1.0 in the jth position and 0.0 elsewhere.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "# load the MNIST database\n",
    "print(\"Loading the MNIST database...\")\n",
    "training_data, validation_data, test_data = [list(d) for d in load_data()]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 7.6200e-02 -5.6645e-01  3.6142e-02  ...  -2.8836e-01 -2.1979e-01  2.0025e-01\n",
      "-8.4550e-01  2.6429e+00 -3.3374e-01  ...   1.3265e+00  8.4115e-01  9.2409e-02\n",
      " 1.0899e+00  2.0696e+00  9.5819e-01  ...  -1.0890e+00  2.9177e-01  2.5380e-01\n",
      "                ...                   ⋱                   ...                \n",
      "-1.1851e+00  6.6195e-01 -5.0461e-01  ...   1.2052e+00 -5.0683e-01  1.5786e-01\n",
      " 5.3238e-01  5.2275e-02 -2.1909e+00  ...  -8.4788e-01 -2.1325e+00 -3.3003e-01\n",
      "-1.6596e-01  5.0059e-03 -1.5762e-01  ...  -2.8192e-01 -9.6807e-01 -5.6724e-01\n",
      "[torch.FloatTensor of size 30x784]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize a Neural Network\n",
    "\n",
    "layer_sizes = [784, 30, 10]  # <-- first entry must be 784, last one must be 10\n",
    "\n",
    "np.random.seed(1234)\n",
    "net = NeuralNetworkMNIST(layer_sizes)\n",
    "\n",
    "print(net.weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained network: got right 988 out of 10000 (accuracy 9.88 pct)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the performance of the untrained network\n",
    "\n",
    "nright = net.evaluate(test_data)\n",
    "print(\"Untrained network: got right %d out of %d (accuracy %.2f pct)\" % (nright, len(test_data), 100*float(nright)/len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: 9052 of 10000 (elapsed time: 31.136235s)\n",
      "Epoch  2: 9197 of 10000 (elapsed time: 30.378844s)\n",
      "Epoch  3: 9225 of 10000 (elapsed time: 30.323463s)\n",
      "Epoch  4: 9313 of 10000 (elapsed time: 29.917347s)\n",
      "Epoch  5: 9371 of 10000 (elapsed time: 30.168319s)\n",
      "Epoch  6: 9344 of 10000 (elapsed time: 29.575896s)\n",
      "Epoch  7: 9386 of 10000 (elapsed time: 30.110790s)\n",
      "Epoch  8: 9423 of 10000 (elapsed time: 30.147428s)\n",
      "Epoch  9: 9445 of 10000 (elapsed time: 29.946958s)\n",
      "Epoch 10: 9412 of 10000 (elapsed time: 30.106696s)\n",
      "Epoch 11: 9398 of 10000 (elapsed time: 29.655067s)\n",
      "Epoch 12: 9466 of 10000 (elapsed time: 29.782650s)\n",
      "Epoch 13: 9437 of 10000 (elapsed time: 30.256443s)\n",
      "Epoch 14: 9414 of 10000 (elapsed time: 30.343114s)\n",
      "Epoch 15: 9402 of 10000 (elapsed time: 30.050161s)\n",
      "Epoch 16: 9396 of 10000 (elapsed time: 29.985712s)\n",
      "Epoch 17: 9459 of 10000 (elapsed time: 29.357427s)\n",
      "Epoch 18: 9487 of 10000 (elapsed time: 29.898439s)\n",
      "Epoch 19: 9470 of 10000 (elapsed time: 29.923440s)\n",
      "Epoch 20: 9461 of 10000 (elapsed time: 29.700854s)\n",
      "Epoch 21: 9437 of 10000 (elapsed time: 29.679841s)\n",
      "Epoch 22: 9453 of 10000 (elapsed time: 30.280495s)\n",
      "Epoch 23: 9472 of 10000 (elapsed time: 30.230954s)\n",
      "Epoch 24: 9478 of 10000 (elapsed time: 30.110664s)\n",
      "Epoch 25: 9465 of 10000 (elapsed time: 29.636722s)\n",
      "Epoch 26: 9470 of 10000 (elapsed time: 38.709536s)\n",
      "Epoch 27: 9493 of 10000 (elapsed time: 47.702991s)\n",
      "Epoch 28: 9448 of 10000 (elapsed time: 47.554158s)\n",
      "Epoch 29: 9458 of 10000 (elapsed time: 47.902805s)\n",
      "Epoch 30: 9459 of 10000 (elapsed time: 48.022792s)\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "\n",
    "EPOCHS  = 30\n",
    "BATCHSZ = 10\n",
    "ETA     =  2\n",
    "\n",
    "net.SGD(training_data, EPOCHS, BATCHSZ, ETA, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
