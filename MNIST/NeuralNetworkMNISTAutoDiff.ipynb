{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Neural Network with Autograd\n",
    "<br\\>\n",
    "This notebook implements a Neural Network for recognizing hand written digits, trained using the MNIST database. The backpropagation algorithm is <b>not</b> implemented explicitly. Instead, we build the computational graph associated with the network and calculate the gradients via reverse mode differentiation. We carry out these calculations with the help of `pytorch`.\n",
    "<br\\><br/>\n",
    "In terms of performance, this implementation runs significatly slower (~6x) compared with the one which implements explicitely the BP algorithm. This was a bit unexpected, but may be due to the overhead of setting up a computational graph (dealing with `torch.autograd.Variable` and `torch.autograd.Function`) and passing values among the nodes, compared with just doing matrix multiplication ops (in BP). I did confirm the performace difference by running the three exaples provided at http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#warm-up-numpy and timing them (see the ComparisonNNImplementation notebook). While when comparing these examples the performace penalty is not as pronounced (only about ~2.5x), there is a significant slowdown when replacing BP with reverse mode differentiation.\n",
    "<br\\><br\\>\n",
    "Also see the `NeuralNetworkMNISTBackProp` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import pickle\n",
    "import timeit\n",
    "import torch\n",
    "from torch import Tensor, LongTensor\n",
    "from torch.autograd import Variable\n",
    "import line_profiler\n",
    "%load_ext line_profiler\n",
    "\n",
    "dtype = torch.DoubleTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkMNISTAutoGrad(object):\n",
    "    \"\"\"\n",
    "    A Neural Network designed to be used with the MNIST database. \n",
    "    Gradients are calculated using reverse mode differentiation on\n",
    "    the computational graphs associated with the network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layersz):\n",
    "        \"\"\"\n",
    "        Pass in a list of layer sizes (layersz[0]/layersz[-1] are the input/output layers).\n",
    "        The size of this list is the number of layers in the network.\n",
    "        Since we are using this network with the MNIST database, the input layer must\n",
    "        be of size 784 = 28 x 28 (the number of pixels of each image). Also, the ouput\n",
    "        layer must be of size 10 (to represent 0...9).\n",
    "        \n",
    "        PARAMETERS:\n",
    "        layersz -- list of layer sizes    \n",
    "        \"\"\"\n",
    "        if layersz [0] != 784: raise RuntimeError('The size of the input layer must be 784')\n",
    "        if layersz[-1] !=  10: raise RuntimeError('The size of the output layer must be 10')\n",
    "        \n",
    "        self.nlayers = len(layersz)\n",
    "        self.layersz = layersz\n",
    "\n",
    "        # Initializes biases and weights with random values from a N(0,1) distribution.\n",
    "        # The following convention is used for weights: \n",
    "        #    w[i,j] denotes the weight associated with the connection from neuron\n",
    "        #   'j' in the previous layer to the neuron 'i' in the current layer.\n",
    "                \n",
    "        # NOTE: we use the numpy random number generator as opposed to the pytorch one,\n",
    "        #       so we can compare with other implementations, for debugging\n",
    "        bs = [np.random.randn(i, 1) for i in layersz[1:]]\n",
    "        ws = [np.random.randn(i, j) for i, j in zip(layersz[1:], layersz[:-1])]\n",
    "\n",
    "        self.biases  = [Variable(torch.from_numpy(b).type(dtype), requires_grad=True) for b in bs]\n",
    "        self.weights = [Variable(torch.from_numpy(w).type(dtype), requires_grad=True) for w in ws]\n",
    "\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        Propagates a given input vector forward through the network and returns the output.\n",
    "        PARAMETERS:\n",
    "        a Variable/Tensor of size [784, 1], representing a digit image from the MNIST database\n",
    "        RETURN:\n",
    "        a Variable/Tensor of size [10, 1], a one-hot representation of the network output (0..9)       \n",
    "        \"\"\"        \n",
    "        if a.data.shape != (self.layersz[0], 1):\n",
    "            raise RuntimeError('Input array has wrong shape - must be (784, 1)')\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = torch.sigmoid(torch.mm(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def SGD(self, training_data, epochs, batchsz, eta, test_data=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using batch stochastic gradient descent.  \n",
    "        The network weights and biases are updated as the result of running this method.\n",
    "        Both 'training_data' and 'test_data' are lists of tuples, each tuple being an\n",
    "        example - the first element is the network input, the second is the target output.\n",
    "        For both data sets, the first tuple element is an MNIST digit image, represented \n",
    "        as a Variable/Tensor of size [784, 1].\n",
    "        The target output (the digit associated with the image) is represented as a one-hot\n",
    "        Variable/Tensor of size [10, 1] in 'training_data', and as the actual digit (0..9) in \n",
    "        'test_data'.\n",
    "        \n",
    "        PARAMETERS:\n",
    "        training_data -- list of tuples representing training inputs and the desired outputs  \n",
    "        epochs        -- for how many epochs to train the network\n",
    "        batchsz       -- the size of each batch of training example (this is *stochastic* GD)\n",
    "        eta           -- the learning rate\n",
    "        test_data     -- used to evaluate the performace of the network at the end of each epoch\n",
    "        \"\"\"\n",
    "        for j in range(epochs):\n",
    "            start_time = timeit.default_timer()\n",
    "                \n",
    "            # break up the training data into batches\n",
    "            np.random.shuffle(training_data)\n",
    "            batches = [training_data[k:k+batchsz] for k in range(0, len(training_data), batchsz)]\n",
    "            \n",
    "            # SGD means that we update weights/biases based on gradients calculated\n",
    "            # using only a batch of training examples (as opposed to the entire training data) \n",
    "            for batch in batches:\n",
    "                                \n",
    "                # calculate the (stochastic) gradient\n",
    "                # below 'x' represents an input image, 'y' the associated digit\n",
    "                for x, y in batch:\n",
    "                    out  = self.feedforward(x)\n",
    "                    loss = 0.5 * (out - y).pow(2).sum()  # <-- quadratic loss function\n",
    "                    loss.backward()                      # <-- replaces the backpropagation algorithm\n",
    "                    \n",
    "                # update weights/biases in the direction of the stochastic gradient\n",
    "                eta_scaled = float(eta)/len(batch)\n",
    "                for w in self.weights: w.data -= eta_scaled * w.grad.data\n",
    "                for b in self.biases:  b.data -= eta_scaled * b.grad.data\n",
    "                \n",
    "                for b in self.biases:  b.grad.data.zero_() \n",
    "                for w in self.weights: w.grad.data.zero_() \n",
    "                \n",
    "            dt = timeit.default_timer() - start_time\n",
    "            if test_data: print(\"Epoch %2d: %d of %d (elapsed time: %fs)\" % (j+1, self.evaluate(test_data), len(test_data), dt))\n",
    "            else:         print(\"Epoch %2d complete  (elapsed time: %fs)\" % (j+1), dt)\n",
    "\n",
    "                \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Evaluates the performance of the neural network on a given data set.\n",
    "        This dataset consists of a list of tuples, each tuple being an example: \n",
    "        the first tuple entry is an image encoded as an is a Variable/Tensor of \n",
    "        size [784, 1] the second one is a Variable storing the digit the image\n",
    "        represents (as 0..9). \n",
    "        PARAMETERS:\n",
    "        test_data -- dataset used for evaluating network performance\n",
    "        RETURNS:\n",
    "        number of correct answers on the given dataset\n",
    "        \"\"\"\n",
    "        # when passing an image through the network, the output is a one-hot vector\n",
    "        # we use the 'argmax' to convert this vector to the 0..9 digit it represents        \n",
    "        test_results = [(np.argmax(self.feedforward(x).data.numpy()), y) for (x, y) in test_data]        \n",
    "        return sum(int(x == y.data[0]) for (x, y) in test_results)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the MNIST database...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# where to find the file storing the MNIST database\n",
    "MNIST_DATA_FILEPATH = \"mnist.pkl.gz\"\n",
    "\n",
    "def load_data_raw():\n",
    "    \"\"\"\n",
    "    Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The 'training_data' is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the 'training_data' tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The 'validation_data' and 'test_data' are similar, except\n",
    "    each contains only 10,000 images.\n",
    "    \"\"\"\n",
    "    f = gzip.open(MNIST_DATA_FILEPATH, 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Repackages the data returned by 'load_data_raw' in a format\n",
    "    more convenient for using with the neural network.\n",
    "    \n",
    "    Return a tuple (training_data, validation_data, test_data).\n",
    "\n",
    "    'training_data'   is a list of 50,000 2-tuples (x, y)\n",
    "    'validation_data' is a list of 10,000 2-tuples (x, z)\n",
    "    'test_data'       is a list of 10,000 2-tuples (x, z)\n",
    "\n",
    "    'x' is a Variable/Tensor of size [784, 1] containing the input image.\n",
    "    'y' is a Variable/Tensor of size [ 10, 1] representing the digit encoded\n",
    "        by 'x' (it has 0 entries with the exception of one 1 in the position\n",
    "        of the digit represented by 'x')\n",
    "    'z' is a Variable storing just the digit represented by 'x'\n",
    "    \"\"\"\n",
    "    tr_d, va_d, te_d = load_data_raw()\n",
    "    \n",
    "    training_in = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_in = [Variable(torch.from_numpy(x).type(dtype), requires_grad=False) for x in training_in]\n",
    "    \n",
    "    training_out = [asvector(y) for y in tr_d[1]]\n",
    "    training_out = [Variable(torch.from_numpy(y).type(dtype), requires_grad=False) for y in training_out]\n",
    "\n",
    "    training_data = zip(training_in, training_out)\n",
    "\n",
    "    # ------------\n",
    "    \n",
    "    validation_in = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_in = [Variable(torch.from_numpy(x).type(dtype), requires_grad=False) for x in validation_in]  \n",
    "\n",
    "    validation_out  = [Variable(torch.from_numpy(np.array([y])).type(dtype), requires_grad=False) for y in va_d[1]]\n",
    "    validation_data = zip(validation_in, validation_out)\n",
    "\n",
    "    # ------------\n",
    "\n",
    "    test_in = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_in = [Variable(torch.from_numpy(x).type(dtype), requires_grad=False) for x in test_in]\n",
    "\n",
    "    test_out  = [Variable(torch.from_numpy(np.array([y])).type(dtype), requires_grad=False) for y in te_d[1]]\n",
    "    test_data = zip(test_in, test_out)\n",
    "\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "\n",
    "def asvector(j):\n",
    "    \"\"\"Create vector of shape (10, 1) with 1.0 in the jth position and 0.0 elsewhere.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "# load the MNIST database\n",
    "print(\"Loading the MNIST database...\")\n",
    "training_data, validation_data, test_data = [list(d) for d in load_data()]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize a Neural Network\n",
    "\n",
    "layer_sizes = [784, 30, 10]  # <-- first entry must be 784, last one must be 10\n",
    "np.random.seed(1234)\n",
    "net = NeuralNetworkMNISTAutoGrad(layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained network: got right 988 out of 10000 (accuracy 9.88 pct)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the performance of the untrained network\n",
    "\n",
    "nright = net.evaluate(test_data)\n",
    "print(\"Untrained network: got right %d out of %d (accuracy %.2f pct)\" % (nright, len(test_data), 100*float(nright)/len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the network, the following params result in ~94.5% accurancy.<br/>\n",
    "```\n",
    "EPOCHS  = 20 \n",
    "BATCHSZ = 10 \n",
    "ETA     =  2 \n",
    "np.random.seed(1234)\n",
    "```\n",
    "We seed the random number generator explicitely to ensure reproducible results.\n",
    "While most of the time the network will converge to the quoted accuracy, once in a while it can get stuck around 80% and refuse to improve (the only thing that differs between these runs is the random number init).\n",
    "Notice that seeding the RNG must happen before we instantiate the network (because the weights are init to random values in the constructor).\n",
    "<br\\><br\\>\n",
    "This implementation of the NN is supposed to produce results identical to the one that explicitely implements the BP algorithm (that's why the current version uses `numpy.random` as opposed to the `pytorch` randomizer, so we can seed the two implementations with the same value). In practice we see small difference in results after about 5 epochs. This is most likely due to rounding errors and is not a reason for concern. The two networks have been tested side by side and it is fair to say they are performing identicaly, up to numerical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: 8947 of 10000 (elapsed time: 33.241560s)\n",
      "Epoch  2: 9181 of 10000 (elapsed time: 33.424959s)\n",
      "Epoch  3: 9240 of 10000 (elapsed time: 33.082751s)\n",
      "Epoch  4: 9280 of 10000 (elapsed time: 32.460828s)\n",
      "Epoch  5: 9334 of 10000 (elapsed time: 33.700519s)\n",
      "Epoch  6: 9363 of 10000 (elapsed time: 33.408844s)\n",
      "Epoch  7: 9404 of 10000 (elapsed time: 33.234715s)\n",
      "Epoch  8: 9424 of 10000 (elapsed time: 33.339761s)\n",
      "Epoch  9: 9447 of 10000 (elapsed time: 33.311969s)\n",
      "Epoch 10: 9429 of 10000 (elapsed time: 34.273902s)\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "\n",
    "EPOCHS  = 10\n",
    "BATCHSZ = 10\n",
    "ETA     =  2\n",
    "\n",
    "net.SGD(training_data, EPOCHS, BATCHSZ, ETA, test_data)\n",
    "\n",
    "# to profile the network, run the line below instead of the one above\n",
    "# set EPOCHS = 1\n",
    "#%lprun -f net.SGD net.SGD(training_data, EPOCHS, BATCHSZ, ETA, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
